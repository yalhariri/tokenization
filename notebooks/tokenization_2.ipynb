{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbdf8ab",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "By Youssef Al Hariri\n",
    "\n",
    "This notebook aims to simplify the tokenization concept in the Text processing and LLM modelling.\n",
    "This notebook has been written based on several resources:\n",
    "\n",
    "1) https://platform.openai.com/tokenizer\n",
    "2) https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "2) https://udlbook.github.io/udlbook/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06b1c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e081b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b6f7d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24912, 2375, 261, 117525, 55894]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"hello world aaaaaaaaaaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbee131",
   "metadata": {},
   "source": [
    "We have created the following example to check how tiktoken (a fast open-source tokenizer by OpenAI) tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cbb651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_colorful_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Display tokens with alternating background colors.\n",
    "\n",
    "    Args:\n",
    "        tokens: Iterable of token values. Items may be:\n",
    "            - str: token text\n",
    "            - bytes or bytearray: will be decoded with UTF-8 (errors='replace')\n",
    "            - int: token id (will be converted to str)\n",
    "    Behavior:\n",
    "        Wraps each token in a colored <span> (4 pastel colors cycled) and attempts\n",
    "        to render the result as HTML in a Jupyter environment. If HTML display is\n",
    "        unavailable, falls back to printing the plain tokens.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    colors = [\"#ffe6e6\", \"#e6f2ff\", \"#e6ffe6\", \"#fff2e6\"]  # pastel red, blue, green, orange\n",
    "    \n",
    "    for i, t in enumerate(tokens):\n",
    "        if isinstance(t, (bytes, bytearray)):\n",
    "            t = t.decode(\"utf-8\", errors=\"replace\")\n",
    "        \n",
    "        if str(t) == '\\n':\n",
    "            spans.append('<br>')\n",
    "            continue\n",
    "        \n",
    "        safe_t = str(t).replace(' ', '_').replace('<', '&lt;').replace('>', '&gt;')\n",
    "        color = colors[i % len(colors)]\n",
    "        spans.append(f'<span style=\"background:{color}; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">{safe_t}</span>')\n",
    "\n",
    "    html = '<div style=\"font-family:monospace; line-height:1.6\">' + ' '.join(spans) + '</div>'\n",
    "\n",
    "    try:\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(html))\n",
    "    \n",
    "    except Exception:\n",
    "        print(' '.join(tokens))\n",
    "        \n",
    "def tokenize_string(text: str) -> tuple[list[int], list[str]]:\n",
    "    \"\"\"\n",
    "    Encode text using tiktoken and display tokens with alternating background colors.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[int], list[str]]: \n",
    "            - encoded_tokens: list of token ids (ints) produced by enc.encode(text).\n",
    "            - tokens: list of decoded token strings (each element is str).\n",
    "\n",
    "    Side effects:\n",
    "        - Prints a short summary showing the original text, encoded token ids,\n",
    "          and decoded token strings.\n",
    "        - Attempts to render the tokens as colored HTML spans in a Jupyter\n",
    "          environment (falls back to plain printing if HTML display isn't available).\n",
    "\n",
    "    Notes:\n",
    "        - Relies on a global `enc` tiktoken tokenizer defined earlier in the notebook.\n",
    "        - Decoding of single-token bytes uses enc.decode_single_token_bytes(...).\n",
    "    \"\"\"\n",
    "    encoded_tokens = enc.encode(text)\n",
    "    tokens = [enc.decode_single_token_bytes(token) for token in encoded_tokens]\n",
    "\n",
    "    # Print plain representation (ids and token strings)\n",
    "    print(f\"Original Text: {text}\\n\")\n",
    "    \n",
    "    print(f\"\\nEncoded Tokens:\")\n",
    "\n",
    "    print_colorful_tokens(encoded_tokens)\n",
    "    \n",
    "    print(f\"\\nDecoded Tokens:\")\n",
    "    print_colorful_tokens(tokens)\n",
    "\n",
    "    return encoded_tokens, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1aa9ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: A sailor went to sea sea sea\n",
      "to see what he could see see see\n",
      "but all that he could see see see\n",
      "was the bottom of the deep blue sea sea sea\n",
      "\n",
      "\n",
      "Encoded Tokens:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family:monospace; line-height:1.6\"><span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">32</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">187469</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">5981</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">316</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">198</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">935</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1412</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">501</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">2023</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">198</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">8293</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">722</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">484</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">501</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">2023</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">1921</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">198</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9844</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">290</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">8725</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">328</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">290</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">8103</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9861</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">9624</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Tokens:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family:monospace; line-height:1.6\"><span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">A</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sailor</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_went</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_to</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span> <br> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">to</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_what</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_he</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_could</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <br> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">but</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_all</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_that</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_he</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_could</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_see</span> <br> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">was</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_the</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_bottom</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_of</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_the</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_deep</span> <span style=\"background:#ffe6e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_blue</span> <span style=\"background:#e6f2ff; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span> <span style=\"background:#e6ffe6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span> <span style=\"background:#fff2e6; padding:2px 6px; margin:2px; border-radius:4px; display:inline-block\">_sea</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text= \"hello world aaaaaaaaaaaa\"\n",
    "text = \"\"\"A sailor went to sea sea sea\n",
    "to see what he could see see see\n",
    "but all that he could see see see\n",
    "was the bottom of the deep blue sea sea sea\"\"\"\n",
    "encoded_tokens, tokens = tokenize_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa944e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
