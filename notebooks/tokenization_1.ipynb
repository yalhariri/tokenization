{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d71e4c",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "By Youssef Al Hariri\n",
    "\n",
    "This notebook aims to simplify the tokenization concept in the Text processing and LLM modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d08fc",
   "metadata": {},
   "source": [
    "\n",
    "## Learning objectives:\n",
    "- Practice different tokenization strategies (split, regex, NLTK, spaCy, SentencePiece, Hugging Face).\n",
    "- Interactively compare tokenizer outputs side-by-side.\n",
    "- Extract and view slides from the lecture PPTX at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce0fa4",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units (tokens). A token is an instance of a sequence of characters. \n",
    "\n",
    "For example, here is how the simple form of tokenization works:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc123aa4",
   "metadata": {},
   "source": [
    "## 1) Install the required libraries:\n",
    "\n",
    "This step is not always required. You need to install the required library when an error message shown. It is always better to activate a virtual environment before installing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8888ec0",
   "metadata": {},
   "source": [
    "## Quick setup and install hints\n",
    "\n",
    "If you run this notebook locally or in a colab, you may need to install a few packages. Recommended commands:\\n\n",
    "\n",
    "- For the interactive widgets: `pip install ipywidgets`\\n\n",
    "- For spaCy: `pip install spacy` and `python -m spacy download en_core_web_sm`\\n\n",
    "- For Hugging Face tokenizers: `pip install transformers tokenizers`\\n\n",
    "- For NLTK (if not already): `pip install nltk` and then run `nltk.download('punkt')` in Python.\\n\n",
    "\n",
    "The next cell has the required lines to install the libraries. You can simply uncomment these lines and run them. The examples below attempt safe imports and will print a friendly message if a package is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77de322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/yalha/teaching/tokenization/.venv/lib/python3.13/site-packages (0.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets\n",
    "# !pip install transformers tokenizers\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e97624",
   "metadata": {},
   "source": [
    "## 2) Let us define the example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde65ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: @Hassani truly loves #AIresearch, but ðŸ¤– & ðŸ§  make him go 'hmm...?'\n"
     ]
    }
   ],
   "source": [
    "sentence = \"@Hassani truly loves #AIresearch, but ðŸ¤– & ðŸ§  make him go 'hmm...?'\"\n",
    "print('Sentence:', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd67b01",
   "metadata": {},
   "source": [
    "## 3) Let us exercise different tools and methods for tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c50e8",
   "metadata": {},
   "source": [
    "#### A) Practice the methods:\n",
    "\n",
    "##### 1. Using the str.split() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43262928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (split): ['@Hassani', 'truly', 'loves', '#AIresearch,', 'but', 'ðŸ¤–', '&', 'ðŸ§ ', 'make', 'him', 'go', \"'hmm...?'\"]\n"
     ]
    }
   ],
   "source": [
    "tokens_split = sentence.split()\n",
    "print(f'Tokens (split): {tokens_split}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3672db",
   "metadata": {},
   "source": [
    "##### 2. Using regex:\n",
    "\n",
    "We need to use the re library with findall function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d906e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (regex): ['Hassani', 'truly', 'loves', 'AIresearch', 'but', 'make', 'him', 'go', 'hmm']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokens_regex = re.findall(r'\\w+', sentence)\n",
    "print(f'Tokens (regex): {tokens_regex}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37530a",
   "metadata": {},
   "source": [
    "Notice that the extracted are only the alphabetical characters. \n",
    "\n",
    "\n",
    "Below, we will use another method that handles different unicodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950b3963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (regex): ['@', 'Hassani', 'truly', 'loves', '#', 'AIresearch', ',', 'but', 'ðŸ¤–', '&', 'ðŸ§ ', 'make', 'him', 'go', \"'\", 'hmm', '.', '.', '.', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "tokens_regex = re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "print(f'Tokens (regex): {tokens_regex}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d652e",
   "metadata": {},
   "source": [
    "##### 3. Using NLTK:\n",
    "\n",
    "A method by using word_tokenize from the NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a179456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (NLTK): ['@', 'Hassani', 'truly', 'loves', '#', 'AIresearch', ',', 'but', 'ðŸ¤–', '&', 'ðŸ§ ', 'make', 'him', 'go', \"'hmm\", '...', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    tokens_nltk = nltk.word_tokenize(sentence)\n",
    "    print(f'Tokens (NLTK): {tokens_nltk}')\n",
    "except ImportError:\n",
    "    print('NLTK is not installed. Run: pip install nltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669671a",
   "metadata": {},
   "source": [
    "Another method by using TweetTokenizer from the NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3841163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (NLTK TweetTokenizer): ['@Hassani', 'truly', 'loves', '#AIresearch', ',', 'but', 'ðŸ¤–', '&', 'ðŸ§ ', 'make', 'him', 'go', \"'\", 'hmm', '...', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    tokens_nltk = nltk.TweetTokenizer().tokenize(sentence)\n",
    "    print(f'Tokens (NLTK TweetTokenizer): {tokens_nltk}')\n",
    "except ImportError:\n",
    "    print('NLTK is not installed. Run: pip install nltk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8565b5",
   "metadata": {},
   "source": [
    "Notice how different methods handle the same word, such as the mention and the hashtag.\n",
    "\n",
    "\n",
    "##### 4. Using SpaCy:\n",
    "SpaCy is a powerful, open-source Python library designed for advanced Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a97b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy tokens: ['@Hassani', 'truly', 'loves', '#', 'AIresearch', ',', 'but', 'ðŸ¤–', '&', 'ðŸ§ ', 'make', 'him', 'go', \"'\", 'hmm', '...', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except Exception:\n",
    "        # fallback: create blank English model (will still tokenize)\n",
    "        nlp = spacy.blank('en')\n",
    "    doc = nlp(sentence)\n",
    "    print('spaCy tokens:', [t.text for t in doc])\n",
    "except ImportError:\n",
    "    print('spaCy is not installed. Run: pip install spacy && python -m spacy download en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73875c",
   "metadata": {},
   "source": [
    "##### 5. SentencePiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ccc21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: @Hassani truly loves #AIresearch, but ðŸ¤– & ðŸ§  make him go 'hmm...?'\n",
      "Encoded text: ['â–', '@', 'H', 'as', 's', 'an', 'i', 'â–tru', 'ly', 'â–l', 'o', 've', 's', 'â–', '#', 'A', 'I', 're', 'se', 'ar', 'ch', ',', 'â–but', 'â–', 'ðŸ¤–', 'â–', '&', 'â–', 'ðŸ§ ', 'â–make', 'â–him', 'â–go', 'â–', \"'\", 'h', 'm', 'm', '.', '.', '.', '?', \"'\"]\n",
      "Encoded ids: [4, 0, 165, 185, 8, 84, 25, 751, 41, 169, 21, 125, 8, 4, 0, 194, 100, 56, 91, 48, 92, 5, 52, 4, 0, 4, 0, 4, 0, 188, 89, 121, 4, 32, 60, 26, 26, 6, 6, 6, 152, 32]\n",
      "Decoded text: @Hassani truly loves #AIresearch, but ðŸ¤– & ðŸ§  make him go 'hmm...?'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/test_model.model')\n",
    "encoded_text = sp.encode(sentence, out_type=str)\n",
    "encoded_ids = sp.encode(sentence)\n",
    "decoded_text = sp.decode(encoded_text)\n",
    "print(f'Original text: {sentence}')\n",
    "print(f'Encoded text: {encoded_text}')\n",
    "print(f'Encoded ids: {encoded_ids}')\n",
    "print(f'Decoded text: {decoded_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a6aff",
   "metadata": {},
   "source": [
    "##### 6. Hugging Face / subword tokenizers\n",
    "\n",
    "Subword tokenizers are used by modern LLMs. This demo uses a small pretrained tokenizer (e.g., distilbert) to show how text is broken into subwords and token ids. If transformers isn't installed the cell will instruct how to install it.\n",
    "\n",
    "A. AutoTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d664473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (HuggingFace): ['@', 'hassan', '##i', 'truly', 'loves', '#', 'aires', '##ear', '##ch', ',', 'but', '[UNK]', '&', '[UNK]', 'make', 'him', 'go', \"'\", 'hmm', '.', '.', '.', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens_hf = tokenizer.tokenize(sentence)\n",
    "    print(f'Tokens (HuggingFace): {tokens_hf}')\n",
    "except ImportError:\n",
    "    print('Transformers is not installed. Run: pip install transformers tokenizers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfffb1e",
   "metadata": {},
   "source": [
    "B. BertTokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495ef7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (HuggingFace BertTokenizer): ['@', 'hassan', '##i', 'truly', 'loves', '#', 'aires', '##ear', '##ch', ',', 'but', '[UNK]', '&', '[UNK]', 'make', 'him', 'go', \"'\", 'hmm', '.', '.', '.', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens_hf = tokenizer.tokenize(sentence)\n",
    "    print(f'Tokens (HuggingFace BertTokenizer): {tokens_hf}')\n",
    "except ImportError:\n",
    "    print('Transformers is not installed. Run: pip install transformers tokenizers')\n",
    "except Exception as e:\n",
    "    print(f'Error loading BertTokenizer: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348c0e0",
   "metadata": {},
   "source": [
    "In both cases, we observe how the name \"Youssef\" is tokenized into three subword pieces: 'you', '##sse', and '##f'. This reflects BERT's WordPiece tokenizer strategy, where uncommon words are broken into known subword units using the ## prefix to indicate _continuation_.\n",
    "\n",
    "Additionally, emojis such as ðŸ¤– and ðŸ§  are replaced with the special token [UNK], which stands for \"unknown\", meaning these characters are ___out-of-vocabulary___ (OOV) and not represented in the model's learned embeddings.\n",
    "\n",
    "Below, we display the tokenized output and corresponding token IDs. These IDs are used to retrieve embeddings from the model's vocabulary table and serve as input to the transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406e3dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (HuggingFace AutoTokenizer): ['@', 'hassan', '##i', 'truly', 'loves', '#', 'aires', '##ear', '##ch', ',', 'but', '[UNK]', '&', '[UNK]', 'make', 'him', 'go', \"'\", 'hmm', '.', '.', '.', '?', \"'\"]\n",
      "Token IDs (HuggingFace AutoTokenizer): [101, 1030, 13222, 2072, 5621, 7459, 1001, 9149, 14644, 2818, 1010, 2021, 100, 1004, 100, 2191, 2032, 2175, 1005, 17012, 1012, 1012, 1012, 1029, 1005, 102]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    tokens_hf = tokenizer.tokenize(sentence)\n",
    "    tokens_ids = tokenizer(sentence)['input_ids']\n",
    "    print(f'Tokens (HuggingFace AutoTokenizer): {tokens_hf}')\n",
    "    print(f'Token IDs (HuggingFace AutoTokenizer): {tokens_ids}')\n",
    "except ImportError:\n",
    "    print('Transformers is not installed. Run: pip install transformers tokenizers')\n",
    "except Exception as e:\n",
    "    print(f'Error loading AutoTokenizer: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b834a",
   "metadata": {},
   "source": [
    "## Comparison and notes\n",
    "\n",
    "- String split is fast but naive and will keep punctuation attached.\n",
    "- regex-based splitting can remove punctuation but may drop tokens like hashtags or mentions depending on the pattern.\n",
    "- NLTK and spaCy provide linguistically informed tokenization (better for downstream NLP).\n",
    "- Hugging Face tokenizers break words into subwords and produce ids for model input (essential for LLMs).\n",
    "\n",
    "When choosing a tokenizer, consider downstream task, vocabulary, and whether the model you're using expects specific subword tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb029e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0aba7334ab4fe38f919b3a7cf6094e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Textarea(value=\"@Hassani truly loves #AIresearch, but ðŸ¤– & ðŸ§  make him go 'hmm...?â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526b3fcd53474d4aa412aa988f3a8033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive tokenizer playground (ipywidgets)\n",
    "# This cell creates a small UI to compare tokenizers side-by-side. It handles missing packages gracefully.\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, Markdown, clear_output\n",
    "    import re\n",
    "    # safe imports for tokenizers\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "    except Exception:\n",
    "        word_tokenize = None\n",
    "    try:\n",
    "        import spacy\n",
    "        try:\n",
    "            _nlp = spacy.load('en_core_web_sm')\n",
    "        except Exception:\n",
    "            _nlp = spacy.blank('en')\n",
    "    except Exception:\n",
    "        _nlp = None\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "    except Exception:\n",
    "        AutoTokenizer = None\n",
    "\n",
    "    # Controls\n",
    "    text_in = widgets.Textarea(value=sentence, description='Text:', layout=widgets.Layout(width='100%', height='80px'))\n",
    "    cb_split = widgets.Checkbox(value=True, description='split()')\n",
    "    cb_regex = widgets.Checkbox(value=True, description=r'regex split (\\W+)')\n",
    "    cb_nltk = widgets.Checkbox(value=False, description='NLTK')\n",
    "    cb_spacy = widgets.Checkbox(value=False, description='spaCy')\n",
    "    cb_sentencepiece = widgets.Checkbox(value=False, description='SentencePiece')\n",
    "    cb_hf = widgets.Checkbox(value=False, description='HuggingFace (subword)')\n",
    "    hf_model = widgets.Dropdown(options=['distilbert-base-uncased','bert-base-uncased'], value='distilbert-base-uncased', description='HF model:')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def run_tokenizers(_=None):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            s = text_in.value\n",
    "            results = {}\n",
    "            if cb_split.value:\n",
    "                results['split'] = [f\"\\\"{x}\\\"\" for x in s.split()]\n",
    "            if cb_regex.value:\n",
    "                # use regex \"\\W+\" to split on non-word characters (keeps underscores and letters/digits)\n",
    "                results['regex'] = [f\"\\\"{t}\\\"\" for t in re.split(r'\\W+', s) if t!='']\n",
    "            if cb_nltk.value:\n",
    "                if word_tokenize is not None:\n",
    "                    results['nltk'] = [f\"\\\"{x}\\\"\" for x in word_tokenize(s)]\n",
    "                else:\n",
    "                    results['nltk'] = 'NLTK not installed (pip install nltk; then nltk.download(punkt))'\n",
    "            if cb_spacy.value:\n",
    "                if _nlp is not None:\n",
    "                    results['spacy'] = [f\"\\\"{t.text}\\\"\" for t in _nlp(s)]\n",
    "                else:\n",
    "                    results['spacy'] = 'spaCy not installed (pip install spacy; python -m spacy download en_core_web_sm)'\n",
    "            if cb_sentencepiece.value:\n",
    "                try:\n",
    "                    sp = spm.SentencePieceProcessor()\n",
    "                    sp.load('models/test_model.model')\n",
    "                    results['sentencepiece'] = [f\"\\\"{x}\\\"\" for x in sp.encode(s, out_type=str)]\n",
    "                    results['sentencepiece_ids'] = [f\"\\\"{x}\\\"\" for x in sp.encode(s)]\n",
    "                except Exception as e:\n",
    "                    results['sentencepiece'] = f'Failed to load SentencePiece model: {e}'\n",
    "            if cb_hf.value:\n",
    "                if AutoTokenizer is not None:\n",
    "                    try:\n",
    "                        tok = AutoTokenizer.from_pretrained(hf_model.value)\n",
    "                        results['hf_tokens'] = [f\"\\\"{x}\\\"\" for x in tok.tokenize(s)]\n",
    "                        # tokenizer(...) returns a dict-like BatchEncoding; 'input_ids' gives token ids\n",
    "                        results['hf_ids'] = [f\"\\\"{x}\\\"\" for x in tok(s)['input_ids']]\n",
    "                    except Exception as e:\n",
    "                        results['hf_tokens'] = f'Failed to load HF tokenizer: {e}'\n",
    "                else:\n",
    "                    results['hf_tokens'] = 'Transformers not installed (pip install transformers tokenizers)'\n",
    "\n",
    "            cols = list(results.keys())\n",
    "            if not cols:\n",
    "                display(Markdown('No tokenizers selected.'))\n",
    "                return\n",
    "            # Build a simple HTML table showing each tokenizer's output in a column\n",
    "            html_lines = []\n",
    "            # header row\n",
    "            html_lines.append('<table style=\\\"width:100%; border-collapse:collapse; font-family: monospace;\\\">')\n",
    "            html_lines.append('<tr>')\n",
    "            html_lines.append(f'<th style=\\\"text-align:left; border-bottom:1px solid #ccc; padding:6px\\\">Method</th><th style=\\\"text-align:left; border-bottom:1px solid #ccc; padding:6px\\\">Value</th>')\n",
    "            html_lines.append('</tr>')\n",
    "            for c in cols:\n",
    "                html_lines.append('<tr>')\n",
    "                val = results[c]\n",
    "                if isinstance(val, list):\n",
    "                    out_txt = ' '.join(str(x) for x in val)\n",
    "                else:\n",
    "                    out_txt = str(val)\n",
    "                html_lines.append(f'<th style=\\\"text-align:left; border-bottom:1px solid #ccc; padding:6px\\\">{c}</th>')\n",
    "                # escape < and > to avoid HTML issues in tokens\n",
    "                out_txt = out_txt.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                html_lines.append(f'<td style=\\\"vertical-align:top; text-align:left; padding:6px\\\"><pre>{out_txt}</pre></td>')\n",
    "                \n",
    "                html_lines.append('</tr>')\n",
    "            # data row\n",
    "            html_lines.append('</table>')\n",
    "            display(HTML('\\n'.join(html_lines)))\n",
    "\n",
    "    run_btn = widgets.Button(description='Run tokenizers', button_style='primary')\n",
    "    run_btn.on_click(run_tokenizers)\n",
    "\n",
    "    controls = widgets.HBox([widgets.VBox([text_in, widgets.HBox([run_btn])]), widgets.VBox([cb_split, cb_regex, cb_nltk, cb_spacy, cb_sentencepiece, cb_hf, hf_model])])\n",
    "    display(controls, out)\n",
    "\n",
    "except Exception as e:\n",
    "    print('ipywidgets not available or failed to render widgets. Install via: pip install ipywidgets')\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06775907",
   "metadata": {},
   "source": [
    "I hope this explains the concept of Tokenization with practical examples of different methods and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
